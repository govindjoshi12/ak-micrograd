{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGHatCI51JP"
      },
      "source": [
        "# micrograd exercises\n",
        "\n",
        "1. watch the [micrograd video](https://www.youtube.com/watch?v=VMj-3S1tku0) on YouTube\n",
        "2. come back and complete these exercises to level up :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFt6NKOz6iBZ"
      },
      "source": [
        "## section 1: derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3Jx9fCXl5xHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.336362190988558\n"
          ]
        }
      ],
      "source": [
        "# here is a mathematical expression that takes 3 inputs and produces one output\n",
        "from math import sin, cos\n",
        "\n",
        "def f(a, b, c):\n",
        "  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n",
        "\n",
        "a = 2.0\n",
        "b = 3.0\n",
        "c = 4.0\n",
        "f_abc = f(a, b, c)\n",
        "print(f_abc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$ f(a, b, c) = -a^3 + \\sin(3b) - \\frac{1}{c} + b^{2.5} - a^{0.5} $\n",
        "\n",
        "- $\\frac{\\partial f}{\\partial a} = -3a^2 - 0.5a^{-0.5}$\n",
        "\n",
        "- $\\frac{\\partial f}{\\partial b} = 3\\cos{3b} + 2.5b^{1.5}$\n",
        "\n",
        "- $\\frac{\\partial f}{\\partial c} =  c^{-2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dellF_A(a, b, c):\n",
        "    return (-3 * (a ** 2)) - (0.5 * (a ** -0.5))\n",
        "\n",
        "# math.cos(x)\n",
        "def dellF_B(a, b, c):\n",
        "    return (3 * cos(3 * b)) + (2.5 * (b**1.5))\n",
        "\n",
        "def dellF_C(a, b, c):\n",
        "    return c ** -2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qXaH59eL9zxf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\n",
            "OK for dim 2: expected 0.0625, yours returns 0.0625\n"
          ]
        }
      ],
      "source": [
        "# write the function df that returns the analytical gradient of f\n",
        "# i.e. use your skills from calculus to take the derivative, then implement the formula\n",
        "# if you do not calculus then feel free to ask wolframalpha, e.g.:\n",
        "# https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29\n",
        "\n",
        "def gradf(a, b, c):\n",
        "  return [dellF_A(a, b, c), dellF_B(a, b, c), dellF_C(a, b, c)] # todo, return [df/da, df/db, df/dc]\n",
        "\n",
        "# expected answer is the list of\n",
        "ans = [-12.353553390593273, 10.25699027111255, 0.0625]\n",
        "yours = gradf(2, 3, 4)\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_27n-KTA9Qla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353559348809995\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.256991666679482\n",
            "OK for dim 2: expected 0.0625, yours returns 0.062499984743169534\n"
          ]
        }
      ],
      "source": [
        "# now estimate the gradient numerically without any calculus, using\n",
        "# the approximation we used in the video.\n",
        "# you should not call the function df from the last cell\n",
        "\n",
        "# -----------\n",
        "numerical_grad = [0, 0, 0] # TODO\n",
        "# -----------\n",
        "\n",
        "# Pick h < 1e-5\n",
        "h = 1e-6\n",
        "\n",
        "numerical_grad[0] = (f(a + h, b, c) - f_abc) / h\n",
        "numerical_grad[1] = (f(a, b + h, c) - f_abc) / h\n",
        "numerical_grad[2] = (f(a, b, c + h) - f_abc) / h\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Symmetric derivative is the average of the left and right derivatives. The \"traditional\"\n",
        "numeric approximation shifts the function to the right by adding h, but we can also\n",
        "shift the function to the left by subtracting h.\n",
        "\n",
        "- $L_{right} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\\\\$\n",
        "\n",
        "- $L_{left} = \\lim_{h \\to 0} \\frac{f(x) - f(x - h)}{h}$\n",
        "\n",
        "- $L_{symm} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x - h)}{2h}$\n",
        "\n",
        "From wikipedia: if a function is differentiable, it is also symmetrically differentiable.\n",
        "However, if it's not differentiable, it may still be symmetrically differentiable\n",
        "- For example, the absolute value function $f(x) = |x|$ is not differentiable at $x=0$. However, it is symm. diffable. \n",
        "\n",
        "For differentiable functions, the symm. diff. provides a better numerical approximation than the usual derivative quotient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BUqsGb5o_h2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553391353245\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027401572\n",
            "OK for dim 2: expected 0.0625, yours returns 0.06250000028629188\n",
            "\n",
            "With h=1e-06\n",
            "Regular approx. error: 5.958216721779763e-06, Symmetric approx. error: 7.599716411732516e-10\n",
            "Regular approx. error: 1.3955669331267018e-06, Symmetric approx. error: 2.9031710369054053e-09\n",
            "Regular approx. error: 1.5256830465659732e-08, Symmetric approx. error: 2.8629187909245957e-10\n"
          ]
        }
      ],
      "source": [
        "# there is an alternative formula that provides a much better numerical\n",
        "# approximation to the derivative of a function.\n",
        "# learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative\n",
        "# implement it. confirm that for the same step size h this version gives a\n",
        "# better approximation.\n",
        "\n",
        "# -----------\n",
        "numerical_grad2 = [0, 0, 0] # TODO\n",
        "# -----------\n",
        "\n",
        "numerical_grad2[0] = (f(a + h, b, c) - f(a - h, b, c)) / (2* h)\n",
        "numerical_grad2[1] = (f(a, b + h, c) - f(a, b - h, c)) / (2* h)\n",
        "numerical_grad2[2] = (f(a, b, c + h) - f(a, b, c - h)) / (2* h)\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")\n",
        "\n",
        "\n",
        "# Symmetric approximation is much better\n",
        "print(f\"\\nWith h={h}\")\n",
        "for i in range(3):\n",
        "  regDiff = abs(ans[i] - numerical_grad[i])\n",
        "  symDiff = abs(ans[i] - numerical_grad2[i])\n",
        "  print(f\"Regular approx. error: {regDiff}, Symmetric approx. error: {symDiff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tklF9s_4AtlI"
      },
      "source": [
        "## section 2: support for softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nAPe_RVrCTeO"
      },
      "outputs": [],
      "source": [
        "# Value class starter code, with many functions taken out\n",
        "from math import exp, log\n",
        "\n",
        "class Value:\n",
        "\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other): # exactly as in the video\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  # ------\n",
        "  # re-implement all the other functions needed for the exercises below\n",
        "  # your code here\n",
        "  # TODO\n",
        "  # ------\n",
        "  \n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    # c = a * b\n",
        "    # dc/da = b, dc/db = a\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "  \n",
        "  # unary minus\n",
        "  def __neg__(self):\n",
        "    return self * -1\n",
        "\n",
        "  def exp(self):\n",
        "    val = exp(self.data)\n",
        "    out = Value(val, (self,), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += val * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "  \n",
        "  def log(self):\n",
        "    out = Value(log(self.data), (self,), 'log')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (self.data ** -1) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "\n",
        "    b = self.data\n",
        "    e = other.data\n",
        "    out = Value(pow(b, e), (self, other), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = (e * (b ** (e - 1))) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "    # next = self / other\n",
        "    # dNext/dSelf = d/dSelf[self / other]\n",
        "    # Quotient rule:\n",
        "    # (otherPrime * self - selfPrime * other) / other^2\n",
        "    # But I don't think we can calculate self' and other'\n",
        "\n",
        "    # Can we use symmetric differentiation?\n",
        "    # next(self, other) = self / other\n",
        "    # dNext/dSelf = (next(self + h, other) - next(self - h, other)) / 2h\n",
        "\n",
        "    # No, it didn't work. We'll try the route from the video\n",
        "\n",
        "    return self * (other ** -1)\n",
        "\n",
        "  def backward(self): # exactly as in video\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I only had to implement the `exp`, `mul`, `pow`, and `div` operators. \n",
        "- At first I thought I have to implement a specific softmax operator and the softmax derivative.\n",
        "- It theoretically makes sense that simply implementing the atomic operations involved in softmax are enough, but I am still skeptical about how they work without implementing the softmax derivative itself. \n",
        "- Why is the list passed into softmax called `logits`? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VgWvwVQNAvnI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Value(data=0.04177257051535045), Value(data=0.839024507462532), Value(data=0.00565330266221633), Value(data=0.11354961935990122)]\n",
            "[0.0, 0.0, 0.0, 0.0]\n",
            "2.1755153626167147\n",
            "[0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
            "OK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\n",
            "OK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\n",
            "OK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\n",
            "OK for dim 3: expected -0.8864503806400986, yours returns -0.8864503806400986\n"
          ]
        }
      ],
      "source": [
        "# without referencing our code/video __too__ much, make this cell work\n",
        "# you'll have to implement (in some cases re-implemented) a number of functions\n",
        "# of the Value object, similar to what we've seen in the video.\n",
        "# instead of the squared error loss this implements the negative log likelihood\n",
        "# loss, which is very often used in classification.\n",
        "\n",
        "# this is the softmax function\n",
        "# https://en.wikipedia.org/wiki/Softmax_function\n",
        "def softmax(logits):\n",
        "  counts = [logit.exp() for logit in logits]\n",
        "  denominator = sum(counts)\n",
        "  out = [c / denominator for c in counts]\n",
        "  return out\n",
        "\n",
        "# this is the negative log likelihood loss function, pervasive in classification\n",
        "logits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
        "probs = softmax(logits)\n",
        "print(probs)\n",
        "\n",
        "loss = -probs[3].log() # dim 3 acts as the label for this input example\n",
        "\n",
        "print(list(logit.grad for logit in logits))\n",
        "\n",
        "loss.backward()\n",
        "print(loss.data)\n",
        "print(list(logit.grad for logit in logits))\n",
        "\n",
        "\n",
        "ans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
        "for dim in range(4):\n",
        "  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Torch Tensors\n",
        "- Can initialize a tensor with a list, and pass in dtype and device to create a tensor of a specific type\n",
        "- `requires_grad=True` if you want the autograd to keep track of this tensor\n",
        "- Can call different operations on a tensor\n",
        "- `t.item()` to get the element from a single element tensor\n",
        "- `t.grad` for gradient\n",
        "- Useful function from an error: `Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "q7ca1SVAGG1S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0418, 0.8390, 0.0057, 0.1135], grad_fn=<DivBackward0>)\n",
            "None\n",
            "tensor(2.1755)\n",
            "tensor([ 0.0418,  0.8390,  0.0057, -0.8865])\n"
          ]
        }
      ],
      "source": [
        "# verify the gradient using the torch library\n",
        "# torch should give you the exact same gradient\n",
        "import torch\n",
        "\n",
        "t = torch.tensor([0.0, 3.0, -2.0, 1.0], requires_grad=True)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# t.requires_grad_(True)\n",
        "# t.retain_grad()\n",
        "\n",
        "# Don't do this\n",
        "# t = t.exp() \n",
        "\n",
        "sm = t.exp() / t.exp().sum()\n",
        "loss = -sm[3].log()\n",
        "\n",
        "# tensors have .grad \n",
        "print(sm)\n",
        "print(t.grad)\n",
        "\n",
        "loss.backward()\n",
        "print(loss.data)\n",
        "\n",
        "print(t.grad)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
