{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micrograd\n",
    "\n",
    "This notebook only contains the necessary micrograd classes from `micrograd.ipynb` (no markdown). \n",
    "- I will make updates and changes (like adding operators to the Value object) here instead of the original file\n",
    "- I also turned this code into python modules that can be used as libraries. If I want to test any new code (ex. Linear Regression module), I can do it here before changing that code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from graphviz import Digraph\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "\n",
    "# These are copied from Part 6 below. You can find much better\n",
    "# commented code there. This is here for a complete all-in-one\n",
    "# implementation of autograd\n",
    "def topologicalSort(root):\n",
    "\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    # recursive dfs\n",
    "    def topoHelper(node, topo, visited):\n",
    "        if node in visited:\n",
    "            return \n",
    "        visited.add(node)\n",
    "\n",
    "        for c in node._prev:\n",
    "            topoHelper(c, topo, visited)\n",
    "        topo.append(node)\n",
    "\n",
    "    topoHelper(root, topo, visited)\n",
    "\n",
    "    # returns nodes in reverse topologically sorted order\n",
    "    return topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node in computational graph\n",
    "# children combine to produce current Value\n",
    "class Value:\n",
    "\n",
    "    # input nodes have no children and no operator,\n",
    "    # They are the 'leaves' of the computational graph\n",
    "    \n",
    "    # I believe we are using this \"backwards\" structure\n",
    "    # where inputs are leaves and final outputs are roots\n",
    "    # because we are preparing for backprop\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "\n",
    "        # default _backward function, ex. for a leaf, does nothing\n",
    "        # We set the backward functions of parent nodes when creating them.\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op \n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    # Operators\n",
    "\n",
    "    # Allow operating on constants (floats that are not wrapped in Value objects)\n",
    "    # If val is not of type Value, assumes that it is a float\n",
    "    def processOperand(self, val):\n",
    "        return val if isinstance(val, Value) else Value(val)\n",
    "\n",
    "    # a + b\n",
    "    # left value will be self, right value will be other\n",
    "    def __add__(self, val):\n",
    "        val = self.processOperand(val)\n",
    "        out = Value(self.data + val.data, (self, val), '+')\n",
    "\n",
    "        # When parent calls _backward, its gradient is propagated\n",
    "        # to us (its' children), and we update our gradients\n",
    "        # using the chain rule. \n",
    "        def _backward():\n",
    "            # local derivative * dFinal / dOut\n",
    "            # Addition function: simply \"route\" parent gradient to me.\n",
    "            # We do += because this node could be involved in other calculations\n",
    "            # and thus could have multiple parents. The partial derivative in that case\n",
    "            # is the sum of each dParent/dChild derivative. \n",
    "            self.grad += 1.0 * out.grad  \n",
    "            val.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # right add to constant. Ex. 2 + a\n",
    "    def __radd__(self, val):\n",
    "        return self + val\n",
    "        \n",
    "    def __sub__(self, val):\n",
    "        val = self.processOperand(val)\n",
    "        out = Value(self.data - val.data, (self, val), '-')\n",
    "\n",
    "        # Similar to __add__, but slight differences\n",
    "        # next = self - val\n",
    "        # deriv w.r.t val = -1\n",
    "        # deriv w.r.t self = 1\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad  \n",
    "            val.grad += -1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # right subtract from constant. Ex. 2 - a\n",
    "    def __rsub__(self, val):\n",
    "        return Value(val) - self\n",
    "\n",
    "    def __mul__(self, val):\n",
    "        val = self.processOperand(val)\n",
    "        out = Value(self.data * val.data, (self, val), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += val.data * out.grad\n",
    "            val.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # Ex. 2 * a\n",
    "    def __rmul__(self, val):\n",
    "        return self * val\n",
    "    \n",
    "    # Only supporting int/floats for val for now\n",
    "    # TODO: Support Value objects\n",
    "    def __pow__(self, val):\n",
    "        assert(isinstance(val, (int, float)))\n",
    "\n",
    "        out = Value(self.data ** val, (self,), f'**{val}')\n",
    "\n",
    "        # power rule\n",
    "        def _backward():\n",
    "            self.grad += (val * (self.data ** (val - 1))) * out.grad\n",
    "        out._backward =_backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # python3 uses __truediv__ for / \n",
    "    #          and __floordiv__ for //\n",
    "    # Division implemented with __pow__ operator\n",
    "    def __truediv__(self, val):\n",
    "        out = self * (val ** -1.0)\n",
    "        out._op = '/'\n",
    "        return out\n",
    "\n",
    "    # Originally implemented division operator.\n",
    "    def atomic_div(self, val):\n",
    "        val = self.processOperand(val)\n",
    "        out = Value(self.data / val.data, (self, val), '/')\n",
    "\n",
    "        # Similar to __mul__ but slight differences\n",
    "        # next = self * (val)^-1\n",
    "        # dNext/dVal = -1 * self * (val)^-2\n",
    "        # dNext/dSelf = 1 / val\n",
    "        def _backward():\n",
    "            self.grad += (1.0 / val.data) * out.grad\n",
    "            val.grad += (-1.0 * self.data * (val.data ** -2.0)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        val = math.exp(self.data)\n",
    "        out = Value(val, (self,), 'exp')\n",
    "        \n",
    "        # derivative of e^x = e^x\n",
    "        # next = exp(self)\n",
    "        # dNext/dSelf = exp(self)\n",
    "        def _backward():\n",
    "            self.grad += val * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        expVal = (self * 2.0).exp()\n",
    "        val = (expVal - 1.0) / (expVal + 1.0)\n",
    "\n",
    "        # Each atomic function used in the expression above will \n",
    "        # provided its own _backward function, with division \n",
    "        # being the first function differntiated during backprop\n",
    "        return val  \n",
    "\n",
    "    # We can write out the tanh function explicitly by implementing\n",
    "    # __sub__, __div__, and exp. But it's not necessary to have \n",
    "    # these atomic computational pieces. You can have something as \n",
    "    # simple as an addition operator or something arbitrarily complex, \n",
    "    # but the most important thing is that you know how to differentiate\n",
    "    # the function you implement so you compute the \"local\" partial derivative.\n",
    "    def atomic_tanh(self):\n",
    "        x = self.data\n",
    "        val = (math.exp(2.0 * x) - 1.0) / (math.exp(2.0 * x) + 1.0)\n",
    "        out = Value(val, (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1.0 - val ** 2.0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # I can implement this with atomic operations I've already written\n",
    "    def mse(self, val):\n",
    "        val = self.processOperand(val)\n",
    "        out = (val - self) ** 2\n",
    "        return out\n",
    "    \n",
    "    # --- AUTOGRAD ---\n",
    "    \n",
    "    def backward(self):\n",
    "        # can run autograd with any node being the \"global\" function\n",
    "        # against which each variable is differentiated. Just need to \n",
    "        # set my grad to 1 (base case) before running backward\n",
    "        \n",
    "        self.grad = 1 # base case\n",
    "        rev = topologicalSort(self)\n",
    "        for i in range(len(rev) - 1, -1, -1):\n",
    "            rev[i]._backward()\n",
    "\n",
    "# TODO:\n",
    "# Would be very cool to implement more operators, such as log, sigmoid,\n",
    "# ReLU, etc. and other non-linearities, along with things like self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGraph(root):\n",
    "\n",
    "    adjList = {}\n",
    "\n",
    "    frontier = deque([root])\n",
    "    while frontier:\n",
    "        node = frontier.pop()\n",
    "        if not node:\n",
    "            continue\n",
    "        adjList[node] = set()\n",
    "\n",
    "        for c in node._prev:\n",
    "            adjList[node].add(c)\n",
    "            frontier.append(c)\n",
    "    \n",
    "    return adjList\n",
    "\n",
    "def graphVisualizer(root):\n",
    "\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) \n",
    "    # LR: left-to-right (default is vertical)\n",
    "    \n",
    "    adjList = getGraph(root)\n",
    "    for node in adjList:\n",
    "        # shape: record - uses rectangle for data. Without specifiying, defaults to ovals for operators\n",
    "        # { %s | ... } I think this is special syntax for graphviz to obtain the bar separator \n",
    "        dot.node(str(id(node)), \n",
    "                \"{ %s | data: %.4f | grad: %.4f  }\" % (node.label, node.data, node.grad), \n",
    "                shape=\"record\")\n",
    "\n",
    "    # The actual adjList is \"backwards\": the final output is the parent.\n",
    "    # We want to display the forward pass, so we will flip the edges\n",
    "    for node, children in adjList.items():\n",
    "        # id(obj) returns unique integer identifier for obj\n",
    "        uid = str(id(node))\n",
    "        if node._op:\n",
    "            opNodeId = uid + node._op\n",
    "            dot.node(opNodeId, node._op)\n",
    "            dot.edge(opNodeId, uid)\n",
    "            for c in children:\n",
    "                dot.edge(str(id(c)), opNodeId)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single NN Neuron\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, numInputs):\n",
    "\n",
    "        self.weights = [Value(random.uniform(-1, 1)) for _ in range(numInputs)]\n",
    "        self.bias = Value(random.uniform(-1, 1))\n",
    "    \n",
    "    # forward(x)\n",
    "    # x should be a list of ints/floats or Value() objects of size numInputs.\n",
    "    def __call__(self, x):\n",
    "        # assert(len(x) == len(self.weights))\n",
    "\n",
    "        # zip returns an iterator of tuples: (self.weights[i], x[i]) pairs\n",
    "        # sum() takes in an optional starting value\n",
    "        rawActivation = sum((w * x_i for w, x_i in zip(self.weights, x)), start=self.bias)        \n",
    "        return rawActivation.atomic_tanh()\n",
    "\n",
    "    # Identical to __call__, but implemented in an explicit, non-pythonic way\n",
    "    # to clearly see the use of the Value operators we defined\n",
    "    def explicit_call(self, x):\n",
    "        assert(len(x) == len(self.weights))\n",
    "\n",
    "        # Explicit (non-pythonic)\n",
    "        rawActivation = self.bias\n",
    "        for i in range(len(self.weights)):\n",
    "            # This uses the add and multiply operators in Value\n",
    "            # (Not using += because we would need to define __iadd__ for that)\n",
    "            rawActivation = rawActivation + self.weights[i] * x[i]\n",
    "        return rawActivation.atomic_tanh()\n",
    "\n",
    "    # Every module in pytorch has a parameters method that returns all the \n",
    "    # parameters of that module\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Neuron(numInputs={len(self.weights)}, act=tanh)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, inputDim, outputDim):\n",
    "        # the output of each neuron call n(x) is a Value() object\n",
    "        self.inputDim = inputDim\n",
    "        self.neurons = [Neuron(inputDim) for _ in range(outputDim)]\n",
    "    \n",
    "    # x should be list of Value objects with len(x) = inputDim\n",
    "    # It represents the activations of the previous layer.\n",
    "    # The list should either be the list of inputs, or a list generated a layer call\n",
    "    def __call__(self, x):\n",
    "        # if layer only has a single neuron, \n",
    "        return [n(x) for n in self.neurons]\n",
    "    \n",
    "    def parameters(self):\n",
    "        # nested for-loop list comprehension\n",
    "        return [param for neuron in self.neurons for param in neuron.parameters()]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'Layer(inp={self.inputDim}, out={len(self.neurons)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    # format based on pytorch MLP (using list of hiddenDims to specify MLP shape)\n",
    "    def __init__(self, inputDim, hiddenDims=[5]):\n",
    "        self.layers = []\n",
    "        prevDim = inputDim\n",
    "        for dim in hiddenDims:\n",
    "            self.layers.append(Layer(prevDim, dim))\n",
    "            prevDim = dim\n",
    "        \n",
    "    # x should be a list numbers or Values of size inputDim\n",
    "    def __call__(self, x):\n",
    "        # Is there a pythonic way to do this? \n",
    "        transformed = x\n",
    "        for layer in self.layers:\n",
    "            transformed = layer(transformed)\n",
    "        return transformed\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for layer in self.layers for param in layer.parameters()]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self.layers)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "    # params is a list of Value objects\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            # move in opposite direction of gradient\n",
    "            param.data -= self.lr * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*args and **kwargs are special keywords that can be \n",
    "used as function arguments.\n",
    "\n",
    "*args: list of all non-keyword arguments passed to function\n",
    "**kwargs: dictionary of all keyword args passed to the function\n",
    "\n",
    "Your function can still accept other positional and keyword args.\n",
    "*args and **kwargs must be the last function arguments, and they will\n",
    "NOT contain the other arguments specified in the function definition\n",
    "\"\"\"\n",
    "\n",
    "def vprint(verbose=True, *args, **kwargs):\n",
    "    if verbose:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "def predict(model, xs):\n",
    "    return [model(x) for x in xs]\n",
    "\n",
    "def train(model: MLP, xs, ys, epochs=25, lr=0.01, verbose=True):\n",
    "\n",
    "    optimizer = Optimizer(model.parameters(), lr)\n",
    "    loss_per_epoch = []\n",
    "\n",
    "    vprint(verbose, \"Training...\")\n",
    "    for i in range(epochs):\n",
    "        yPred = predict(model, xs)\n",
    "\n",
    "        # batch loss\n",
    "        # loss is just another layer in the computational graph!\n",
    "        loss = sum(pred[0].mse(gold) for pred, gold in zip(yPred, ys))\n",
    "        loss_per_epoch.append(loss.data)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            vprint(verbose, f\"Loss at epoch {i}: {loss.data}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeMlp(mlp: MLP):\n",
    "\n",
    "    dot = Digraph(format='svg', graph_attr={ 'rankdir': 'LR' })\n",
    "\n",
    "    # 1. Add all neurons to the graph\n",
    "    for layerIdx, layer in enumerate(mlp.layers):\n",
    "        for neuronIdx, neuron in enumerate(layer.neurons):\n",
    "            nodePrefix = 'out' if layerIdx == len(mlp.layers) - 1 else 'n'\n",
    "            label = f'{nodePrefix}_{layerIdx},{neuronIdx}'\n",
    "            dot.node(str(id(neuron)), label=label)\n",
    "    \n",
    "    for i in range(len(mlp.layers) - 1, 0, -1):\n",
    "        currLayer = mlp.layers[i]\n",
    "        prevLayer = mlp.layers[i-1]\n",
    "\n",
    "        for neuronIdx, neuron in enumerate(currLayer.neurons):\n",
    "            for prevNeuronIdx, prevNeuron in enumerate(prevLayer.neurons):\n",
    "                dot.edge(str(id(prevNeuron)), str(id(neuron)), \n",
    "                         # f-string truncate float: f'num={value:.2f}'\n",
    "                         f'w_{neuronIdx},{prevNeuronIdx}')\n",
    "    \n",
    "    # There aren't actually any neurons in the network representing the input\n",
    "    # We'll visualize the input here\n",
    "    for inputIdx in range(mlp.layers[0].inputDim):\n",
    "        nodeId = f'{inputIdx}' + str(id(mlp.layers[0]))\n",
    "        dot.node(nodeId, label=f'inp_{inputIdx}')\n",
    "        for neuronIdx, neuron in enumerate(mlp.layers[0].neurons):\n",
    "            dot.edge(nodeId, str(id(neuron)), label=f'w_{neuronIdx},{inputIdx}')\n",
    "\n",
    "    \n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"477pt\" height=\"198pt\"\n",
       " viewBox=\"0.00 0.00 476.68 197.62\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 193.62)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-193.62 472.68,-193.62 472.68,4 -4,4\"/>\n",
       "<!-- 139711480686112 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139711480686112</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"161.64\" cy=\"-39.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.64\" y=\"-35.92\" font-family=\"Times,serif\" font-size=\"14.00\">n_0,0</text>\n",
       "</g>\n",
       "<!-- 139711480678096 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139711480678096</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"292.74\" cy=\"-104.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"292.74\" y=\"-100.92\" font-family=\"Times,serif\" font-size=\"14.00\">n_1,0</text>\n",
       "</g>\n",
       "<!-- 139711480686112&#45;&gt;139711480678096 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139711480686112&#45;&gt;139711480678096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.99,-42.88C208.18,-45.62 228.19,-50.61 244.19,-59.62 254.05,-65.17 254.01,-69.8 262.19,-77.62 263.86,-79.22 265.6,-80.86 267.36,-82.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.33,-85.4 275.05,-89.61 270.08,-80.26 265.33,-85.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.19\" y=\"-63.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,0</text>\n",
       "</g>\n",
       "<!-- 139711480686832 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139711480686832</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"292.74\" cy=\"-50.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"292.74\" y=\"-46.92\" font-family=\"Times,serif\" font-size=\"14.00\">n_1,1</text>\n",
       "</g>\n",
       "<!-- 139711480686112&#45;&gt;139711480686832 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139711480686112&#45;&gt;139711480686832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.01,-28.36C202.41,-21.95 224.79,-16.17 244.19,-21.62 251.19,-23.59 258.19,-26.9 264.57,-30.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"263.1,-33.82 273.42,-36.2 266.84,-27.9 263.1,-33.82\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.19\" y=\"-25.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_1,0</text>\n",
       "</g>\n",
       "<!-- 139711480685920 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139711480685920</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"161.64\" cy=\"-115.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.64\" y=\"-111.92\" font-family=\"Times,serif\" font-size=\"14.00\">n_0,1</text>\n",
       "</g>\n",
       "<!-- 139711480685920&#45;&gt;139711480678096 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139711480685920&#45;&gt;139711480678096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.53,-115.45C207.99,-115.13 227.14,-114.36 244.19,-112.62 247.21,-112.31 250.34,-111.93 253.47,-111.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.04,-114.95 263.41,-110.01 253,-108.03 254.04,-114.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.19\" y=\"-118.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,1</text>\n",
       "</g>\n",
       "<!-- 139711480685920&#45;&gt;139711480686832 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139711480685920&#45;&gt;139711480686832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.49,-104.14C201.85,-95.93 224.38,-84.6 244.19,-74.62 249.25,-72.07 254.61,-69.37 259.83,-66.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.66,-69.73 269.01,-62.1 258.5,-63.48 261.66,-69.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.19\" y=\"-94.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_1,1</text>\n",
       "</g>\n",
       "<!-- 139711485602608 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139711485602608</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"430.98\" cy=\"-77.62\" rx=\"37.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"430.98\" y=\"-73.92\" font-family=\"Times,serif\" font-size=\"14.00\">out_2,0</text>\n",
       "</g>\n",
       "<!-- 139711480678096&#45;&gt;139711485602608 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139711480678096&#45;&gt;139711485602608</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.97,-99.03C340.45,-95.36 364.99,-90.5 386.01,-86.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"386.87,-89.73 396,-84.35 385.51,-82.86 386.87,-89.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.28\" y=\"-98.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,0</text>\n",
       "</g>\n",
       "<!-- 139711480686832&#45;&gt;139711485602608 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139711480686832&#45;&gt;139711485602608</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.97,-56.21C340.45,-59.88 364.99,-64.74 386.01,-68.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.51,-72.38 396,-70.89 386.87,-65.51 385.51,-72.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.28\" y=\"-69.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,1</text>\n",
       "</g>\n",
       "<!-- 0139711480676848 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>0139711480676848</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"30.55\" cy=\"-101.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"30.55\" y=\"-97.92\" font-family=\"Times,serif\" font-size=\"14.00\">inp_0</text>\n",
       "</g>\n",
       "<!-- 0139711480676848&#45;&gt;139711480686112 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>0139711480676848&#45;&gt;139711480686112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M39.51,-84.38C47.3,-69.75 60.69,-49.75 79.09,-40.62 92.26,-34.09 108.16,-32.69 122.41,-33.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.18,-36.83 132.43,-34.12 122.73,-29.85 122.18,-36.83\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.09\" y=\"-44.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,0</text>\n",
       "</g>\n",
       "<!-- 0139711480676848&#45;&gt;139711480685920 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0139711480676848&#45;&gt;139711480685920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.84,-104.8C78.75,-106.74 101.88,-109.25 121.36,-111.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.15,-114.86 131.46,-112.46 121.9,-107.9 121.15,-114.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.09\" y=\"-113.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_1,0</text>\n",
       "</g>\n",
       "<!-- 1139711480676848 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1139711480676848</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"30.55\" cy=\"-166.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"30.55\" y=\"-162.92\" font-family=\"Times,serif\" font-size=\"14.00\">inp_1</text>\n",
       "</g>\n",
       "<!-- 1139711480676848&#45;&gt;139711480686112 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1139711480676848&#45;&gt;139711480686112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M56.86,-157.08C77.57,-148.88 104.76,-137.07 113.09,-128.62 126.79,-114.74 122.04,-105.89 131.09,-88.62 135.12,-80.95 139.82,-72.82 144.29,-65.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"147.48,-66.92 149.74,-56.57 141.52,-63.25 147.48,-66.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.09\" y=\"-150.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,1</text>\n",
       "</g>\n",
       "<!-- 1139711480676848&#45;&gt;139711480685920 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1139711480676848&#45;&gt;139711480685920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58.79,-173.53C75.23,-176.3 96.18,-177.26 113.09,-169.62 126.21,-163.69 137.15,-152.17 145.21,-141.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"148.26,-143.02 151.06,-132.8 142.5,-139.05 148.26,-143.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.09\" y=\"-178.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_1,1</text>\n",
       "</g>\n",
       "<!-- 2139711480676848 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2139711480676848</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"30.55\" cy=\"-25.62\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"30.55\" y=\"-21.92\" font-family=\"Times,serif\" font-size=\"14.00\">inp_2</text>\n",
       "</g>\n",
       "<!-- 2139711480676848&#45;&gt;139711480686112 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>2139711480676848&#45;&gt;139711480686112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.19,-12.84C68.79,-4.23 92.64,4.4 113.09,-2.62 122.05,-5.7 130.62,-11.23 137.96,-17.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.1,-20.11 145.95,-24 140.68,-14.81 136.1,-20.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.09\" y=\"-6.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_0,2</text>\n",
       "</g>\n",
       "<!-- 2139711480676848&#45;&gt;139711480685920 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2139711480676848&#45;&gt;139711480685920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.87,-39.09C72.64,-54.27 108.21,-79.06 132.93,-96.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"130.98,-99.21 141.18,-102.05 134.98,-93.46 130.98,-99.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.09\" y=\"-84.42\" font-family=\"Times,serif\" font-size=\"14.00\">w_1,2</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f111d2d4460>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(3, [2, 2, 1])\n",
    "visualizeMlp(mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
