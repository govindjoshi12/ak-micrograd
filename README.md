### Micrograd
This is my implementation of Andrej Karpathy's micrograd, a scalar-valued automatic
differentiation library and tiny neural network library. I have added a few features
such as a ReLU activation, a linear regressor module, and have a made a few other 
changes.

In general, you may notice differences between Andrej's implementation and mine. 
This is because I attempted to create my own solutions to problems while working through 
the video, using the video as an answer key of sorts. 